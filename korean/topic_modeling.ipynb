{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 사용할 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 파일 불러오기\n",
    "- data_dir: 파일 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_dir.split('.')[-1] in ['txt', 'csv']:\n",
    "    data = pd.read_csv(data_dir)\n",
    "elif data_dir.split('.')[-1] in ['xlsx', 'xls']:\n",
    "    data = pd.read_excel(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 사용할 Text가 저장된 Column 지정\n",
    "text_col: Column 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = '사람문장1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Tokenize\n",
    "stopwords: 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['입니다','완전','전부','같아요','대한', '있어서', '있는', '약간', '있어요', '살짝', '적당히', '그냥', '있어', '역시', '모두', '있습니다', '다만', '보고', '같은', '있고', '편이', '같습니다', '좀더', 'jpg', '첨부파일', '때문', '일단', '리뷰', '이서', '그런지', '이상', '가장', '바로', '이건', '사서', '충분히', '안나', '없는', '같네요', '없어서', '있네요', '있었어요', '기도', '없고', '있는데', '거의', '무엇', '하니', '있으면', '없네요', '없어요', '전혀', '추합니다', '이런', '같아서', '아니라', '워낙', '같아', '여기', '뭔가', '해도', '있을', '있던', '비주', '위해', '우선', '불리', '있으니', '있지만', '대로', '사실', '같은데', 'jpeg', '없을', '있었습니다', '있게', '있었으면', '기지', '있음',' 같고', '자꾸', '있다니', '없어', '그런', '그것', '짐해', '수도', '미가', '다가', '인지', '있었는데', '듭니', '만해', '없습니다', '이기', '없었어요', '그닥', '그게', '내기', '편입', '런가', '끼리', '기고', '혹시', '그랬어요', '그거', '질도', '서도', '있었지만', '처럼', '이예', '문해', '고요', '이면', '이나', '알도', '그럴', '이구', '한수', '입니당', '그렇고', '거리', '있었네요', '만하', '가요', '있구요', '어요', '있는거', '주시', '수가', '없는데', '나니', '번은', '는걸', '요건', '어떤', '야해요', '있을걸', '단지', '아예', '성하게', '있다면', '있다는', '고해', '드네', '양장', '없었는데', '로만', '있었음', '있다', '리오', '시기', '어도', '비도', '점점', '없지만', '보이', '차라리', '하라', '그래요', '그렇지', '같기도', '거나', '등등', '더더', '나', '것', '내', '그', '후', '다른', '안', '우리', '이', '오늘', '내일', '다음', '이전']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사\n",
    "#tags = ['Noun', 'Alpha']\n",
    "\n",
    "# 명사, 형용사\n",
    "tags = ['Noun']\n",
    "\n",
    "\n",
    "def Tokenizer(text):\n",
    "    morphs = Okt.pos(text)\n",
    "    if len(morphs) > 0:\n",
    "        pos = []\n",
    "\n",
    "        for x in morphs:\n",
    "            if len(x) > 1:\n",
    "                word, tag = x\n",
    "                if(word in stopwords): \n",
    "                    continue\n",
    "                if tag in tags:\n",
    "                    pos.append(word)\n",
    "    else:\n",
    "        pos = ['nan']\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = [Tokenizer(text) for text in data[text_col]]\n",
    "Token[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = [list(bigrams(token)) for token in Token]\n",
    "merged = list(itertools.chain(*bigram))\n",
    "bigram_count = collections.Counter(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 합성어 전처리\n",
    "N: bigram 합성어 선택 개수  \n",
    "comb_words: 추가 합성어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "comb_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_bigrams = bigram_count.most_common(N)\n",
    "selected_bigrams = [bg for (bg, cnt) in selected_bigrams]\n",
    "comb_words = [[''.join(bg), ' '.join(bg)] for bg in selected_bigrams] + comb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform_words = [' ' + 'TRSF' * (i+1) + ' ' for i in range(len(comb_words))]\n",
    "Transform_words[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_transform(Text, word_list, to_word):\n",
    "    for word in word_list:\n",
    "        Text = str(Text).replace(word,to_word)\n",
    "    return Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(comb_words)):\n",
    "    transformed_text = [Word_transform(txt, comb_words[i], Transform_words[i]) for txt in data[text_col]]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 재 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['Noun', 'Alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = [Tokenizer(text) for text in transformed_text]\n",
    "Token[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 합성어 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word_restore(Text, word1, word2):\n",
    "    transformed_list = []\n",
    "    for word in Text:\n",
    "        if word1 in word:\n",
    "            word = word2\n",
    "        transformed_list.append(word)\n",
    "    return transformed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_words = [word_list[0] for word_list in comb_words]\n",
    "for i in range(len(restore_words)):\n",
    "    Token = [Word_restore(token, Transform_words[i][1:-1], restore_words[i]) for token in Token] \n",
    "Token[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Option 설정\n",
    "- min_count: 특정 개수 이하 단어는 제거\n",
    "- N_grams: N_gram 수 (1: Uni-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 3\n",
    "N_grams = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) LDA 구성 요소 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(token, n):\n",
    "    n_grams = ngrams(token, n)\n",
    "    return [' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_grams > 1:\n",
    "    gram_tokens = [get_ngrams(token, N_grams) for token in Token]\n",
    "else:\n",
    "    gram_tokens = Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word=corpora.Dictionary(gram_tokens)\n",
    "id2word.filter_extremes(no_below = min_count)\n",
    "texts = gram_tokens\n",
    "corpus=[id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 최적의 Topic 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.coherencemodel.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    \n",
    "    x = range(start, limit, step) \n",
    "    plt.plot(x, coherence_values) \n",
    "    plt.xlabel(\"Num Topics\") \n",
    "    plt.ylabel(\"Coherence score\") \n",
    "    plt.legend((\"coherence_values\"), loc='best') \n",
    "    plt.show()\n",
    "    \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_topic_num = 3\n",
    "end_topic_num = 10\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start_topic_num, limit=end_topic_num+1, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_topic_num = range(start_topic_num, end_topic_num+1, step)[coherence_values.index(max(coherence_values))]\n",
    "print(\"Best topic number: {}\".format(best_topic_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 최적의 LDA 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_list[coherence_values.index(max(coherence_values))]\n",
    "topics = best_model.print_topics(num_words=8) \n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) pyLDAvis 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(best_model, corpus, id2word)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 토픽 배정\n",
    "- prob: 각 토픽에 대한 확률을 배정 (True) vs 가장 확률이 높은 토픽 이름만 배정 (False)\n",
    "- save_file: 토픽 배정 결과를 csv파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = False\n",
    "save_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prob:\n",
    "    probability = np.zeros((len(best_model[corpus]), best_topic_num))\n",
    "    for doc in range(len(best_model[corpus])):\n",
    "        for topic, proba in best_model[corpus][doc]:\n",
    "            probability[doc, topic] = proba\n",
    "        \n",
    "    probability = pd.DataFrame(probability)\n",
    "    probability.columns = ['Topic' + str(num) for num in range(1, best_topic_num + 1)]\n",
    "    if save_file:\n",
    "        probability.to_csv('topic_porb.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_assignment = [max(p,key=lambda item: item[1]) for p in best_model[corpus]]\n",
    "lda_topic_assignment = [topic + 1 for (topic, proba) in lda_topic_assignment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "w = collections.Counter(lda_topic_assignment)\n",
    "w = sorted(w.items())\n",
    "keys = [key for (key, value) in w]\n",
    "values = [value for (key, value) in w]\n",
    "plt.bar(['Topic-' + str(key) for key in keys], values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 BerTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Option 설정\n",
    "- nr_topics: 토픽 수 ('auto': 최적)\n",
    "- language: ['english', 'multilingual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_topics = 'auto'\n",
    "language = 'multilingual'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) BerTopic 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=Token, max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BERTopic(embedding_model=\"sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\", \\\n",
    "                 vectorizer_model=vectorizer,\n",
    "                 nr_topics=50,\n",
    "                 top_n_words=10,\n",
    "                 calculate_probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = model.fit_transform(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "model = BERTopic(nr_topics=nr_topics, language=language)\n",
    "preprocessed_text = [' '.join(token) for token in Token]\n",
    "topics, probabilities = model.fit_transform(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_info().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(top_n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fb000299d0be6b292a3563f2cb803716502a839d387e9a8df10d0a410910a21"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bert_eng')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
